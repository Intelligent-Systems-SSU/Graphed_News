"""
ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§ ë° ì¦ê°• ë©”ì¸ ëª¨ë“ˆ - LangGraph ì›Œí¬í”Œë¡œìš° ë²„ì „
"""
import asyncio
from typing import Dict, Any, List, TypedDict

from langgraph.graph import StateGraph, START, END

from core.news_crawler import crawl_news
from core.news_processor import extract_news_content
from core.news_question_generator import generate_questions
from core.news_qa_agent import NewsQnAAgent
from core.news_ka_agent import NewsKnAAgent
from core.news_accumulator import NewsAccumulator


class NewsAnalysisState(TypedDict):
    """ë‰´ìŠ¤ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° ìƒíƒœ"""
    url: str
    crawled_content: str
    extracted_content: Dict[str, Any]
    questions: List[str]
    qa_pairs: List[Dict[str, str]]
    ka_pairs: List[Dict[str, str]]
    final_result: str
    

class NewsAnalysisGraph:
    """LangGraph ê¸°ë°˜ ë‰´ìŠ¤ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°"""
    
    def __init__(self):
        """ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”"""
        self.qa_agent = NewsQnAAgent()
        self.ka_agent = NewsKnAAgent()
        self.accumulator = NewsAccumulator()
        self.app = self._build_workflow_graph()
    
    def _crawl_news_node(self, state: NewsAnalysisState):
        """1ë‹¨ê³„: ë‰´ìŠ¤ í¬ë¡¤ë§ ë…¸ë“œ"""
        print(f"ğŸ•·ï¸ ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§: {state['url']}")
        
        # ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            crawled_content = loop.run_until_complete(crawl_news(state["url"]))
        finally:
            loop.close()
        
        return {"crawled_content": crawled_content}
    
    def _extract_content_node(self, state: NewsAnalysisState):
        """2ë‹¨ê³„: ë‰´ìŠ¤ ë‚´ìš© ì •ì œ ë° í‚¤ì›Œë“œ ì¶”ì¶œ ë…¸ë“œ"""
        print("ğŸ“ ë‰´ìŠ¤ ì •ì œ ë° ì£¼ì œ, í‚¤ì›Œë“œ ì¶”ì¶œ")
        
        extracted_result = extract_news_content(state["crawled_content"])
        
        extracted_content = {
            "content": extracted_result.content,
            "topic": extracted_result.topic,
            "keywords": extracted_result.keywords
        }
        
        print(f"ğŸ“Œ ì£¼ì œ: {extracted_result.topic}")
        print(f"ğŸ” í‚¤ì›Œë“œ: {', '.join(extracted_result.keywords)}")
        
        return {"extracted_content": extracted_content}
    
    def _generate_questions_node(self, state: NewsAnalysisState):
        """3ë‹¨ê³„: ì§ˆë¬¸ ìƒì„± ë…¸ë“œ"""
        print("â“ ì§ˆë¬¸ ìƒì„±")
        
        question_result = generate_questions(state["extracted_content"]["content"])
        questions = question_result.questions if hasattr(question_result, 'questions') else question_result
        
        print(f"ğŸ“‹ ìƒì„±ëœ ì§ˆë¬¸ ìˆ˜: {len(questions)}")
        for i, question in enumerate(questions, 1):
            print(f"  {i}. {question}")
        
        return {"questions": questions}
    
    def _answer_questions_node(self, state: NewsAnalysisState):
        """4ë‹¨ê³„: ì§ˆë¬¸ ë‹µë³€ ë…¸ë“œ"""
        print("ğŸ’¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±")
        
        # ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            qa_pairs = loop.run_until_complete(
                self.qa_agent.process_questions_async(
                    state["extracted_content"]["content"], 
                    state["questions"]
                )
            )
        finally:
            loop.close()
        
        print(f"âœ… ë‹µë³€ ì™„ë£Œ: {len(qa_pairs)}ê°œ ì§ˆë¬¸")
        
        return {"qa_pairs": qa_pairs}
    
    def _answer_keywords_node(self, state: NewsAnalysisState):
        """4ë‹¨ê³„: ì§ˆë¬¸ ë‹µë³€ ë…¸ë“œ"""
        print("ğŸ’¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±")
        
        # ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            ka_pairs = loop.run_until_complete(
                self.ka_agent.process_keywords_async(
                    state["extracted_content"]["content"], 
                    state["extracted_content"]["keywords"]
                )
            )
        finally:
            loop.close()
        
        print(f"âœ… ë‹µë³€ ì™„ë£Œ: {len(ka_pairs)}ê°œ ì§ˆë¬¸")
        
        return {"ka_pairs": ka_pairs}
    
    def _accumulate_results_node(self, state: NewsAnalysisState):
        """5ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ìƒì„± ë…¸ë“œ"""
        print("ğŸ”„ ìµœì¢… ê²°ê³¼ ìƒì„±")
        
        final_result = self.accumulator.process(
            state["extracted_content"],
            state["qa_pairs"],
            state["ka_pairs"]
        )
        
        print("âœ¨ ìµœì¢… ê²°ê³¼ ìƒì„± ì™„ë£Œ")
        
        return {"final_result": final_result}
    
    def _build_workflow_graph(self):
        """ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì¶•"""
        workflow_builder = StateGraph(NewsAnalysisState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow_builder.add_node("crawl_news", self._crawl_news_node)
        workflow_builder.add_node("extract_content", self._extract_content_node)
        workflow_builder.add_node("generate_questions", self._generate_questions_node)
        workflow_builder.add_node("answer_questions", self._answer_questions_node)
        workflow_builder.add_node("answer_keywords", self._answer_keywords_node)
        workflow_builder.add_node("accumulate_results", self._accumulate_results_node)
        
        # ì—£ì§€ ì—°ê²° (ìˆœì°¨ì  ì‹¤í–‰)
        workflow_builder.add_edge(START, "crawl_news")
        workflow_builder.add_edge("crawl_news", "extract_content")
        workflow_builder.add_edge("extract_content", "generate_questions")
        workflow_builder.add_edge("extract_content", "answer_keywords")
        workflow_builder.add_edge("generate_questions", "answer_questions")
        workflow_builder.add_edge(["answer_questions", "answer_keywords"], "accumulate_results")  
        workflow_builder.add_edge("accumulate_results", END)
        
        return workflow_builder.compile()


def analyze_articles(url: str):
    """
    ë‰´ìŠ¤ ê¸°ì‚¬ URLì„ ì²˜ë¦¬í•˜ì—¬ ì¦ê°•ëœ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        url (str): ì²˜ë¦¬í•  ë‰´ìŠ¤ ê¸°ì‚¬ URL
        
    Returns:
        Dict[str, Any]: ì²˜ë¦¬ ê²°ê³¼
    """
    workflow = NewsAnalysisGraph()

    print("ğŸš€ ë‰´ìŠ¤ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° ì‹œì‘")
    print("=" * 50)

    initial_state = {"url": url}
    final_state = workflow.app.invoke(initial_state)

    print("=" * 50)
    print("ğŸ‰ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ")
    
    # ìµœì¢… ê²°ê³¼ ì •ë¦¬
    return {
        "url": url,
        "original_content": final_state["crawled_content"],
        "extracted_content": final_state["extracted_content"],
        "questions": final_state["questions"],
        "qa_pairs": final_state["qa_pairs"],
        "ka_pairs": final_state["ka_pairs"],
        "final_result": final_state["final_result"]
    }


def print_results(results: Dict[str, Any]):
    """ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
    print("\n" + "=" * 60)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼")
    print("=" * 60)
    
    print(f"\nğŸŒ URL: {results['url']}")
    
    print(f"\nğŸ“° ì£¼ì œ: {results['extracted_content']['topic']}")
    
    print(f"\nğŸ” í‚¤ì›Œë“œ: {', '.join(results['extracted_content']['keywords'])}")

    print(f"\nğŸ“ ì›ë³¸ ê¸°ì‚¬ ë‚´ìš©: {results['original_content'][:300]}..." if len(results['original_content']) > 300 else results['original_content'])

    print(f"\nğŸ“ ì •ì œëœ ê¸°ì‚¬ ë‚´ìš©: {results['extracted_content']['content'][:300]}..." if len(results['extracted_content']['content']) > 300 else results['extracted_content']['content'])

    print("\n" + "=" * 60)
    
    print(f"\nâ“ ì§ˆë¬¸ ìˆ˜: {len(results['questions'])}")
    for i, question in enumerate(results['questions'], 1):
        print(f"  {i}. {question}")
    
    print("\nğŸ’¬ ì§ˆì˜ì‘ë‹µ:")
    for i, qa_pair in enumerate(results['qa_pairs'], 1):
        question = list(qa_pair.keys())[0]
        answer = list(qa_pair.values())[0]
        print(f"\n  {i}. Q: {question}")
        print(f"     A: {answer}")


    print("\nğŸ”‘ í‚¤ì›Œë“œ ì„¤ëª…:")
    for i, ka_pair in enumerate(results['ka_pairs'], 1):
        keyword = list(ka_pair.keys())[0]
        explanation = list(ka_pair.values())[0]
        print(f"\n  {i}. í‚¤ì›Œë“œ: {keyword}")
        print(f"     ì„¤ëª…: {explanation}")
        

    print(f"\nğŸ“‹ ìµœì¢… ë³´ê³ ì„œ:")
    final_report = results['final_result']
    print(f"   {final_report}")


if __name__ == "__main__":
    url = "https://n.news.naver.com/mnews/article/421/0008261200"
    
    try:
        results = analyze_articles(url)
        print_results(results)
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()